\chapter{Evaluation}
\label{chapter:evaluation}

In order to evaluate the implemented solution, several tests were done to assess protocol correctness and measure system performance.
In Section~\ref{section:scenarios} we detail our evaluation process and the overall objectives we pretend to achieve in the evaluated scenarios.
Then, in Section~\ref{section:methodology} we describe the methods, tools and challenges we had to perform the evaluation.
Lastly, in Section~\ref{section:results} we present and discuss the solutions evaluation results.

\section{Tests Objectives and scenarios}
\label{section:scenarios}

Our evaluation has an extended focus in the correctness of the implemented protocol.
But we also perform benchmarking of the all implemented solutions, in order to compare them.

Given the requirements presented in Section~\ref{architecture:requirements}, the following metrics were defined:

\begin{itemize}
  \item \textit{Response time for writes} – we will evaluate the system response time, as the number of write requests in the DHT increases;
  \item \textit{Response time for reads} – we will also evaluate the system response time, as the number of read requests in the DHT increases;
    % only error rate? add number of nodes storing the value as a metric also?
  \item \textit{Error rate} – this metric has distinct interpretations for read and write requests. In the case of the write requests, we analyze the number of nodes that stored the value, and define different error categories. In read requests, we consider that the request failed when is impossible to obtain a value.
\end{itemize}

Using these metrics, we performed load tests and functional security tests to the \ac{DHT}.
We performed two different types of load test: \textit{(a)} one client only issuing write requests, (b) one client only issuing read requests.
As summarized in Table~\ref{table:test-scenarios}, we run the tests for the different scenarios using a DHT network with 10 nodes deployed.
We conducted tests for each mechanism and write/read combination, where we varied the rate from 0.1 requests/s up to 100 requests/s in write tests, and from 1 request/s to 100 requests/s in read tests.
Only one node was setup to perform the requests, and each test was repeated 5 times.
The tests were executed over the length of several days, in order to minimize any potential effect related to varying network traffic.

\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{%
  \begin{tabular}{lllll}
    \multicolumn{1}{c}{\textbf{Test \#}} & \textbf{Mechanism} & \multicolumn{1}{c}{\textbf{Test Type}} & \multicolumn{1}{c}{\textbf{\# Number of nodes}} & \multicolumn{1}{c}{\textbf{\# Requests/second}} \\ \hline
    \multicolumn{1}{c}{1} & None (HTTP) & \multicolumn{1}{c}{Write} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{[ 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 40, 100 ]} \\ \hline
    \multicolumn{1}{c}{2} & CA-based (HTTPS) & \multicolumn{1}{c}{Write} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{[ 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 40, 100 ]} \\ \hline
    \multicolumn{1}{c}{3} & IDChain (HTTPS) & \multicolumn{1}{c}{Write} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{[ 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 40, 100 ]} \\ \hline
    \multicolumn{1}{c}{4} & None (HTTP) & \multicolumn{1}{c}{Read} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{[ 1, 2, 5, 10, 20, 40, 100 ]} \\ \hline
    \multicolumn{1}{c}{5} & CA-based (HTTPS) & \multicolumn{1}{c}{Read} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{[ 1, 2, 5, 10, 20, 40, 100 ]} \\ \hline
    \multicolumn{1}{c}{6} & IDChain (HTTPS) & \multicolumn{1}{c}{Read} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{[ 1, 2, 5, 10, 20, 40, 100 ]} \\ \hline
  \end{tabular}%
}
\caption{Test scenarios.}
\label{table:test-scenarios}
\end{table}

We also defined a metric for the smart contract evaluation:

\begin{itemize}
  \item \textit{Monetary cost} – we will also measure the cost of executing each smart contract function, with a variable number of entities.
\end{itemize}

This test was executed in a local machine, by calculating the \textit{gas} cost of executing every smart contract function in the \ac{EVM}.

\section{Tests methodology}
\label{section:methodology}

% TODO: section summary

\subsection{DHT Nodes deployment}

The DHT nodes were deployed on Digital Ocean\footnote{https://www.digitalocean.com/}, distributed across several different datacenters.

We deployed 9 nodes, one node for each of the following datacenters: New York City 1 (NYC1), New York City 3 (NYC3), San Francisco 1 (SFO1), Toronto 1 (TOR1), London 1 (LON1), Frankfurt 1 (FRA1), Bengalore 1 (BLR1), Amsterdam 2 (AM2) and Amesterdam 2 (AM3).
The ninth node which correspond to the benchmark client was located in Lisbon.

The node that was defined as bootstrap node in all nodes configuration was the node located in the NYC1 datacenter.

All the nodes used a \acp{VM} with the same specs: 1vCPU, 512 MB RAM, 20 GB SSD disk space, running Ubuntu 16.04.3 x64.
The nodes used Node.js v8.2.1, PostgreSQL 9.5.8 and Ethereum testrpc client v4.1.3.

The benchmarking client was run on a server with Dual Intel Xeon E5-2640@2.00GHz CPU with a total of 32 cores, 128GB of RAM and running Debian 8.2.
The versions of the software used were equal to the Digital Ocean nodes.

\subsection{Tests configuration and tooling}

In order to perform the load test evaluation of systems, was necessary to build a set of scripts.
We built two different scripts for write and read requests.
The write request script receives three parameters: DHT mechanism to use, total number of requests and demanded request rate per second.
This script was ran using the following process:

\begin{enumerate}
  \item Deploy all 9 nodes;
  \item Run the script in benchmarking node with the given parameters.
  \item When the script finishes, delete the all nodes cached data;
  \item Repeat the process.
\end{enumerate}

Is necessary to delete the node cached data between runs, in order to the tests to run in a clean state DHT, withou any data written to.
The script outputs each request start/completion time and number of nodes that stored the written value in a CSV file.

The read request script receives three parameters: DHT mechanism to use, total number of requests and a list of demanded request rates per second.
This script - on contrary to the write script - in each execution will perform several test iterations, increasing the demanded request rate, according to the received list of demanded requests.
We choose this approach in read script because it's not necessary to clear the node data between executions, and allow us to perceive the system performance with an increasing number of requests.

In Tables~\ref{table:total-requests-demanded-rate-writes} and~\ref{table:total-requests-demanded-rate-reads} we present the demanded request rate and total number of requests that were done, in write script and read script executions, respectively.

\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccccc}
  \textbf{Demanded Request Rate (req/s)} & 0.1 & 0.2 & 0.5 & 1 & 2 & 5 & 10 & 20 & 40 & 100 \\ \hline
  \textbf{\# of Requests per run} & 720 & 1440 & 3600 & 3600 & 3600 & 20000 & 20000 & 20000 & 20000 & 20000 \\ \hline
  \textbf{Total Requests} & 3600 & 7200 & \multicolumn{3}{c}{18000} & \multicolumn{5}{c}{100000} \\ \hline
\end{tabular}%
}
\caption{Number of write requests performed according to demanded request rate.}
\label{table:total-requests-demanded-rate-writes}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{cccccccc}
  \textbf{Demanded Request Rate (req/s)} & 1 & 2 & 5 & 10 & 20 & 40 & 100 \\ \hline
  \textbf{\# of Requests per run} & \multicolumn{7}{c}{10000} \\ \hline
  \textbf{Total Requests} & \multicolumn{7}{c}{50000} \\ \hline
\end{tabular}
\caption{Number of read requests performed according to demanded request rate.}
\label{table:total-requests-demanded-rate-reads}
\end{table}

In order to write values to the DHT, we pre-generated several keys by hashing an increasing integer counter, using a SHA-1 function.
The object we stored under each key in the DHT, was always the same: a 827 bytes JSON file.
This JSON file contains a structure similar to values that the Global Registry stores.

The certificates that were used in the test nodes were created using OpenSSL\footnote{https://www.openssl.org/} and cfssl\footnote{https://github.com/cloudflare/cfssl} command-line tools, and respect the certificate requirements that we referred in Sections~\ref{implementation:idchain} and~\ref{implementation:ca-based}.

\subsection{Evaluation challenges}

Our benchmarking client presented performance limitations when executing our test scripts, that taken into consideration when analyzing the results obtained.
Since, our DHT is based in Node.js runtime, there is limitation in the number of concurrent connections that can be done.
The node running in the Digital Ocean datacenters, had by default a maximum number of open file descriptors equal to 1024.
We tried to increase the value, but the limitations persisted.
This causes an enormous limitation when executing the tests, because the main bottleneck was the benchmarking client.

\section{Test Results}
\label{section:results}

\subsection{Load tests}

\subsubsection{Write requests}

\begin{figure}
  \centering
  \includegraphics[scale=0.6]{Figures/evaluation/request-rate-writes-linear.pdf}
  \caption{Demanded request rate.}
\label{fig:demanded-request-rate}
\end{figure}

%\begin{table}[]
%\centering
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{cc|c|c|c|c|c|}
%\cline{3-7}
%\textbf{} & \multicolumn{1}{l|}{\textbf{}} & \multicolumn{5}{c|}{\textbf{Response Time (ms)}} \\ \hline
%\multicolumn{1}{|c|}{\textbf{Demanded Request Rate (req/s)}} & \multicolumn{1}{l|}{\textbf{Effective Request Rate (req/s)}} & \textbf{Mean} & \textbf{Max} & \textbf{Min} & \multicolumn{1}{l|}{\textbf{95th percentile}} & \multicolumn{1}{l|}{\textbf{Standard Deviation}} \\ \hline
%\multicolumn{1}{|c|}{0.1} & 1231 & 3231 & 321312 & 31231 & 321312 & 321321321 \\ \hline
%\multicolumn{1}{|c|}{0.2} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{0.5} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{1} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{2} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{5} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{10} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{20} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{40} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{100} &  &  &  &  &  &  \\ \hline
%\end{tabular}%
%}
%\caption{My caption}
%\label{my-label}
%\end{table}

%\begin{figure}
  %\centering
  %\includegraphics[scale=0.7]{Figures/evaluation/stored-ratio-http.pdf}
  %\caption{Vanilla DHT ratio of storing nodes.}
%\label{fig:stored-ratio-http}
%\end{figure}

\subsubsection{Read requests}

\subsection{Smart contract tests}

\subsubsection{Monetary Costs}

