\chapter{Evaluation}
\label{chapter:evaluation}

In order to evaluate the implemented solution, several tests were done to assess protocol correctness and measure system performance.
In Section~\ref{section:scenarios} we detail our evaluation process and the overall objectives we pretend to achieve in the evaluated scenarios.
Then, in Section~\ref{section:methodology} we describe the methods, tools and challenges we had to perform the evaluation.
Lastly, in Section~\ref{section:results} we present and discuss the solutions evaluation results.

\section{Tests Objectives and scenarios}
\label{section:scenarios}

Our evaluation has an extended focus in the correctness of the implemented protocol.
But we also perform benchmarking of the all implemented solutions, in order to compare them.

Given the requirements presented in Section~\ref{architecture:requirements}, the following metrics were defined:

\begin{itemize}
  \item \textit{Response time for writes} – we will evaluate the system response time, as the number of write requests in the DHT increases;
  \item \textit{Response time for reads} – we will also evaluate the system response time, as the number of read requests in the DHT increases;
    % only error rate? add number of nodes storing the value as a metric also?
  \item \textit{Error rate} – this metric has distinct interpretations for read and write requests. In the case of the write requests, we analyze the number of nodes that stored the value, and define different error categories. In read requests, we consider that the request failed when is impossible to obtain a value.
\end{itemize}

Using these metrics, we performed load tests and functional security tests to the \ac{DHT}.
We performed two different types of load test: \textit{(a)} one client only issuing write requests, (b) one client only issuing read requests.
As summarized in Table~\ref{table:test-scenarios}, we run the tests for the different scenarios using a DHT network with 10 nodes deployed.
We conducted tests for each mechanism and write/read combination, where we varied the rate from 0.1 requests/s up to 100 requests/s in write tests, and from 1 request/s to 100 requests/s in read tests.
Only one node was setup to perform the requests, and each test was repeated 5 times.
The tests were executed over the length of several days, in order to minimize any potential effect related to varying network traffic.

\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{%
  \begin{tabular}{lllll}
    \multicolumn{1}{c}{\textbf{Test \#}} & \textbf{Mechanism} & \multicolumn{1}{c}{\textbf{Test Type}} & \multicolumn{1}{c}{\textbf{\# Number of nodes}} & \multicolumn{1}{c}{\textbf{\# Requests/second}} \\ \hline
    \multicolumn{1}{c}{1} & None (HTTP) & \multicolumn{1}{c}{Write} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{[ 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 40, 100 ]} \\ \hline
    \multicolumn{1}{c}{2} & CA-based (HTTPS) & \multicolumn{1}{c}{Write} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{[ 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 40, 100 ]} \\ \hline
    \multicolumn{1}{c}{3} & IDChain (HTTPS) & \multicolumn{1}{c}{Write} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{[ 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 40, 100 ]} \\ \hline
    \multicolumn{1}{c}{4} & None (HTTP) & \multicolumn{1}{c}{Read} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{[ 1, 2, 5, 10, 20, 40, 100 ]} \\ \hline
    \multicolumn{1}{c}{5} & CA-based (HTTPS) & \multicolumn{1}{c}{Read} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{[ 1, 2, 5, 10, 20, 40, 100 ]} \\ \hline
    \multicolumn{1}{c}{6} & IDChain (HTTPS) & \multicolumn{1}{c}{Read} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{[ 1, 2, 5, 10, 20, 40, 100 ]} \\ \hline
  \end{tabular}%
}
\caption{Test scenarios.}
\label{table:test-scenarios}
\end{table}

We also defined a metric for the smart contract evaluation:

\begin{itemize}
  \item \textit{Monetary cost} – we will also measure the cost of executing each smart contract function, with a variable number of entities.
\end{itemize}

This test was executed in a local machine, by calculating the \textit{gas} cost of executing every smart contract function in the \ac{EVM}.

\section{Tests methodology}
\label{section:methodology}

% TODO: section summary

\subsection{DHT Nodes deployment}

The DHT nodes were deployed on Digital Ocean\footnote{https://www.digitalocean.com/}, distributed across several different datacenters.

We deployed 9 nodes, one node for each of the following datacenters: New York City 1 (NYC1), New York City 3 (NYC3), San Francisco 1 (SFO1), Toronto 1 (TOR1), London 1 (LON1), Frankfurt 1 (FRA1), Bengalore 1 (BLR1), Amsterdam 2 (AM2) and Amesterdam 2 (AM3).
The ninth node which correspond to the benchmark client was located in Lisbon.

The node that was defined as bootstrap node in all nodes configuration was the node located in the NYC1 datacenter.

All the nodes used a \acp{VM} with the same specs: 1vCPU, 512 MB RAM, 20 GB SSD disk space, running Ubuntu 16.04.3 x64.
The nodes used Node.js v8.2.1, PostgreSQL 9.5.8 and Ethereum testrpc client v4.1.3.

The benchmarking client was run on a server with Dual Intel Xeon E5-2640@2.00GHz CPU with a total of 32 cores, 128GB of RAM and running Debian 8.2.
The versions of the software used were equal to the Digital Ocean nodes.

\subsection{Tests configuration and tooling}

In order to perform the load test evaluation of systems, was necessary to build a set of scripts.
We built two different scripts for write and read requests.
The write request script receives three parameters: DHT mechanism to use, total number of requests and demanded request rate per second.
This script was ran using the following process:

\begin{enumerate}
  \item Deploy all 9 nodes;
  \item Run the script in benchmarking node with the given parameters.
  \item When the script finishes, delete the all nodes cached data;
  \item Repeat the process.
\end{enumerate}

Is necessary to delete the node cached data between runs, in order to the tests to run in a clean state DHT, withou any data written to.
The script outputs each request start/completion time and number of nodes that stored the written value in a CSV file.

The read request script receives three parameters: DHT mechanism to use, total number of requests and a list of demanded request rates per second.
This script - on contrary to the write script - in each execution will perform several test iterations, increasing the demanded request rate, according to the received list of demanded requests.
We choose this approach in read script because it's not necessary to clear the node data between executions, and allow us to perceive the system performance with an increasing number of requests.

In Tables~\ref{table:total-requests-demanded-rate-writes} and~\ref{table:total-requests-demanded-rate-reads} we present the demanded request rate and total number of requests that were done, in write script and read script executions, respectively.

\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccccc}
  \textbf{Demanded Request Rate (req/s)} & 0.1 & 0.2 & 0.5 & 1 & 2 & 5 & 10 & 20 & 40 & 100 \\ \hline
  \textbf{\# of Requests per run} & 720 & 1440 & 3600 & 3600 & 3600 & 20000 & 20000 & 20000 & 20000 & 20000 \\ \hline
  \textbf{Total Requests} & 3600 & 7200 & \multicolumn{3}{c}{18000} & \multicolumn{5}{c}{100000} \\ \hline
\end{tabular}%
}
\caption{Number of write requests performed according to demanded request rate.}
\label{table:total-requests-demanded-rate-writes}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{cccccccc}
  \textbf{Demanded Request Rate (req/s)} & 1 & 2 & 5 & 10 & 20 & 40 & 100 \\ \hline
  \textbf{\# of Requests per run} & \multicolumn{7}{c}{10000} \\ \hline
  \textbf{Total Requests} & \multicolumn{7}{c}{50000} \\ \hline
\end{tabular}
\caption{Number of read requests performed according to demanded request rate.}
\label{table:total-requests-demanded-rate-reads}
\end{table}

In order to write values to the DHT, we pre-generated several keys by hashing an increasing integer counter, using a SHA-1 function.
The object we stored under each key in the DHT, was always the same: a 827 bytes JSON file.
This JSON file contains a structure similar to values that the Global Registry stores.

The certificates that were used in the test nodes were created using OpenSSL\footnote{https://www.openssl.org/} and cfssl\footnote{https://github.com/cloudflare/cfssl} command-line tools, and respect the certificate requirements that we referred in Sections~\ref{implementation:idchain} and~\ref{implementation:ca-based}.
In the case of the CA-based mechanism tests, we used a two-tier architecture, with a root CA and a intermediate CA.

\subsection{Evaluation challenges}

Our benchmarking client presented performance limitations when executing our test scripts, that taken into consideration when analyzing the results obtained.
Since, our DHT is based in Node.js runtime, there is limitation in the number of concurrent connections that can be done.
The node running in the Digital Ocean datacenters, had by default a maximum number of open file descriptors equal to 1024.
We tried to increase the value, but the limitations persisted.
This causes an enormous limitation when executing the tests, because the main bottleneck was the benchmarking client.

\section{Test Results}
\label{section:results}
In the following sections we discuss the evaluation results.
The Section~\ref{section:load-tests} focus in read and write load tests, and Section~\ref{section:smart-contract-tests} focus in smart contract execution metrics.

\subsection{Load tests}
\label{section:load-tests}

\subsubsection{Write requests}

The Figures~\ref{fig:demanded-request-rate}–~\ref{fig:stored-ratio-https-bc} represent the write load test evalution results.

The graph from Figure~\ref{fig:demanded-request-rate} represent the effective request rate in function of the demanded request rate, for each of the implemented solutions.
Each point in the graph, illustrate the average request rate of the 5 different test executions performed, for the specific demanded request rate.
In the Figure legend the \textit{http} value represent the vanilla DHT implementation, the \textit{https} value represent the CA-based DHT implementation and \textit{https-bc} represent the IDChain DHT implementation.
For instance, the point (0.1, 0.1), in any of the solutions, represent the average request obtained (0.1 req/s) for 5 repetitions performed for this test, demanding a request rate of 0.1 req/s.
In Figure~\ref{fig:demanded-request-rate} is possible to verify that in the [0.1, 10] request/s demanded request rate, all the three solutions have an approximated equal effective request rate, which means that the client is able to keep up with the demanded request rate.
In the ]10, 100] request/s demand request rate range, thevanilla DHT implementation outperforms the CA-based and IDChain solutions, by being able to keep up more closely with the demanded request rate.
The vanilla DHT implementation is able to perform~$\approx$ 90 requests/second, where the CA-based and IDChain solutions become saturated at~$\approx$ 17 requests/second.
This shows us that the HTTP-based solution is capable of outperforming clearly the HTTPS-based solutions, at the cost of didn't provide any security aspects in the nodes communication.
The message overhead of the HTTPS handshake protocol, limits the request rate capacity of the CA-based and IDChain solutions.
Is important to notice, that could be possible to increase the request rate capacity of the HTTPS-based solutions, if TLS Session Resumption and Keep-Alive connections are activated in HTTPS.
We are also able to conclude that the IDChain solution is a viable solution performance-wise to the CA-based solution.
It seems that increased message header overhead and the requests performed to the IDChain API, doesn't incur in a substantial cost to the effective request rate the client is able to perform, in comparison with the CA-based solution.

\begin{figure}
  \centering
  \includegraphics[scale=0.4]{Figures/evaluation/request-rate-writes-linear.pdf}
  \caption{Demanded request rate.}
\label{fig:demanded-request-rate}
\end{figure}

The graph from Figure~\ref{fig:average-response-time} represent the average request response time in function of the effective request rate, for each of the implemented solutions.
Each point in the graph, illustrate the average request response of the 5 different test executions performed for each of the demanded request rates, in function of the effective request rate.
We can draw similar conclusions to those of the previous discussed figure: the HTTP-based implementation clearly outperforms the HTTPS-based implementations, by having, in average,~$\approx$ 2 times faster response times.
This results of the aforementioned added message overhead of the HTTPS handshake protocol.
The average response time of the CA and IDChain based DHT implementations remain approximately constant, only increasing when an~$\approx$ 15-17 requests/second effective request rate is achieved, which correspond to a demanded request rate in the ]10, 100] requests/second range.
In the case of the HTTP-based implementation, the average request response time also remained constant with an increasing load, only increasing drastically to an average response rate of 23552.88 ms when an effective request rate of 90.83 requests/second - which corresponds to a demanded request rate of 100 request/second - is achieved.
This result is due to the fact that the cli

\begin{figure}
  \centering
  \includegraphics[scale=0.4]{Figures/evaluation/average-response-time.pdf}
  \caption{Average response time.}
  \label{fig:average-response-time}
\end{figure}

\begin{figure}
\centering
\begin{minipage}[t]{.45\textwidth}
\centering
\includegraphics[width=.8\textwidth,scale=0.4]{Figures/evaluation/stored-ratio-http.pdf}
\caption{Number of nodes storing each value, in vanilla DHT.}
\end{minipage}\hfill
\begin{minipage}[t]{.45\textwidth}
\centering
\includegraphics[width=.8\textwidth,scale=0.4]{Figures/evaluation/stored-ratio-https.pdf}
\caption{Number of nodes storing each value, in CA-based solution.}
\end{minipage}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.4]{Figures/evaluation/stored-ratio-https-bc.pdf}
  \caption{Number of nodes storing each value, in IDChain solution.}
  \label{fig:stored-ratio-https-bc}
\end{figure}


%\begin{table}[]
%\centering
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{cc|c|c|c|c|c|}
%\cline{3-7}
%\textbf{} & \multicolumn{1}{l|}{\textbf{}} & \multicolumn{5}{c|}{\textbf{Response Time (ms)}} \\ \hline
%\multicolumn{1}{|c|}{\textbf{Demanded Request Rate (req/s)}} & \multicolumn{1}{l|}{\textbf{Effective Request Rate (req/s)}} & \textbf{Mean} & \textbf{Max} & \textbf{Min} & \multicolumn{1}{l|}{\textbf{95th percentile}} & \multicolumn{1}{l|}{\textbf{Standard Deviation}} \\ \hline
%\multicolumn{1}{|c|}{0.1} & 1231 & 3231 & 321312 & 31231 & 321312 & 321321321 \\ \hline
%\multicolumn{1}{|c|}{0.2} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{0.5} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{1} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{2} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{5} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{10} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{20} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{40} &  &  &  &  &  &  \\ \hline
%\multicolumn{1}{|c|}{100} &  &  &  &  &  &  \\ \hline
%\end{tabular}%
%}
%\caption{My caption}
%\label{my-label}
%\end{table}

%\begin{figure}
  %\centering
  %\includegraphics[scale=0.7]{Figures/evaluation/stored-ratio-http.pdf}
  %\caption{Vanilla DHT ratio of storing nodes.}
%\label{fig:stored-ratio-http}
%\end{figure}

\subsubsection{Read requests}

\subsection{Smart contract tests}
\label{section:smart-contract-tests}

\subsubsection{Monetary Costs}

